{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CDS lab word2vec part 2 (questions)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAA2XCLSQInJ"
      },
      "source": [
        "## Classification with word2vec \n",
        "\n",
        "-- Prof. Dorien Herremans\n",
        "\n",
        "In this second part of the lab, we will be tackling a classification problem by first loading word embeddings and feeding those in a simple classifier. We compare this to naive alternative approaches. \n",
        "\n",
        "During this tutorial, you will need some of the following libraries, let's install them first if you don't have them: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud93NwIrl0Qj"
      },
      "source": [
        "# STUDENT NUMBER: \n",
        "\n",
        "# 1004379"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0AHw5vB1yOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e389b78-7757-4f1f-f6f0-2e91fc99940d"
      },
      "source": [
        "# Use this to install libraries if you find them missing on your system: \n",
        "!pip install bs4 \n",
        "!pip install sklearn\n",
        "!pip install nltk\n",
        "!pip install gensim\n",
        "!pip install lxml"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2yp5xfI15HJ"
      },
      "source": [
        "Now we can import some libraries that we will use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCtMQEnRQjhV"
      },
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import gensim\n",
        "import nltk\n",
        "import lxml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "368EVM3_QInK"
      },
      "source": [
        "### TFIDF with logistic regression\n",
        "\n",
        "#### Preparing the dataset\n",
        "\n",
        "The classification problem at hand is to predict the tag that belongs to a Stack Overflow post. By the way, if you are not familiar with Stack Overflow, do check it out, it is a tremendous help when facing any coding issues. The data from Google BigQuery is available at the github below. If the link does not work you may have to download it manually from github then upload to Colab:\n",
        "\n",
        "https://github.com/dorienh/class_materials/blob/main/datasets/stack-overflow-data.csv\n",
        " \n",
        " We can read it directly into a pandas dataframe. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBLM0vEjyarI"
      },
      "source": [
        "url = \"https://github.com/dorienh/class_materials/blob/main/datasets/stack-overflow-data.csv?raw=true\"\n",
        "\n",
        "df = pd.read_csv(url, encoding = 'latin-1')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMzTr6hCQInM"
      },
      "source": [
        "Let's start by having a look at our data: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3758TYmIQInQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "40b27717-2c4d-49e7-9c3b-530bcb5f9cbd"
      },
      "source": [
        "# only keep data that has a tag (is labeled): \n",
        "df = df[pd.notnull(df['tags'])]\n",
        "\n",
        "# display first ten rows:\n",
        "df.head(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what is causing this behavior  in our c# datet...</td>\n",
              "      <td>c#</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>have dynamic html load as if it was in an ifra...</td>\n",
              "      <td>asp.net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>how to convert a float value in to min:sec  i ...</td>\n",
              "      <td>objective-c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>.net framework 4 redistributable  just wonderi...</td>\n",
              "      <td>.net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>trying to calculate and print the mean and its...</td>\n",
              "      <td>python</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>how to give alias name for my website  i have ...</td>\n",
              "      <td>asp.net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>window.open() returns null in angularjs  it wo...</td>\n",
              "      <td>angularjs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>identifying server timeout quickly in iphone  ...</td>\n",
              "      <td>iphone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>unknown method key  error in rails 2.3.8 unit ...</td>\n",
              "      <td>ruby-on-rails</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>from the include  how to show and hide the con...</td>\n",
              "      <td>angularjs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                post           tags\n",
              "0  what is causing this behavior  in our c# datet...             c#\n",
              "1  have dynamic html load as if it was in an ifra...        asp.net\n",
              "2  how to convert a float value in to min:sec  i ...    objective-c\n",
              "3  .net framework 4 redistributable  just wonderi...           .net\n",
              "4  trying to calculate and print the mean and its...         python\n",
              "5  how to give alias name for my website  i have ...        asp.net\n",
              "6  window.open() returns null in angularjs  it wo...      angularjs\n",
              "7  identifying server timeout quickly in iphone  ...         iphone\n",
              "8  unknown method key  error in rails 2.3.8 unit ...  ruby-on-rails\n",
              "9  from the include  how to show and hide the con...      angularjs"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13ECXgs-xn9p"
      },
      "source": [
        "Our task: predict the tag based on the post content. \n",
        "\n",
        "The size of our word embedding will be chosen based on how many unique words are in the dataset (meaning in the article text or posts): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvAXbtE5QInW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c49f44b-8d1a-48ca-8b89-e482aee851ef"
      },
      "source": [
        "# Count the number of words: \n",
        "df['post'].apply(lambda x: len(x.split(' '))).sum()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10286120"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSifacVqQIna"
      },
      "source": [
        "We have over 10 million words in the data. That's a lot! \n",
        "\n",
        "\n",
        "Let's visualise our dataset: \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-DdZVX9QInb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "06bdf5c5-67a6-43ff-b926-e48160ba86f8"
      },
      "source": [
        "# visualising dataset\n",
        "plt.figure(figsize=(10,4))\n",
        "df.tags.value_counts().plot(kind='bar');"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UwHhJXMQIng"
      },
      "source": [
        "As you can see, the classes are very well balanced.\n",
        "\n",
        "Now let's have a look at the data of the posts ('post' columns) in more detail: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q3m4GxxQInh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab82ca3-16a8-4313-a36f-3fb2bb74e615"
      },
      "source": [
        "print(df['post'].values[10])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when we need interface c# <blockquote>    <strong>possible duplicate:</strong><br>   <a href= https://stackoverflow.com/questions/240152/why-would-i-want-to-use-interfaces >why would i want to use interfaces </a>   <a href= https://stackoverflow.com/questions/9451868/why-i-need-interface >why i need interface </a>    </blockquote>     i want to know where and when to use it     for example    <pre><code>interface idemo {  // function prototype  public void show(); }  // first class using the interface class myclass1 : idemo {  public void show()  {   // function body comes here   response.write( i m in myclass );  }  }  // second class using the interface class myclass2 : idemo {  public void show()   {   // function body comes here   response.write( i m in myclass2 );   response.write( so  what  );  } </code></pre>   these two classes has the same function name with different body. this can be even achieved without interface. then why we need an interface where and when to use it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbSGI0FQQInu"
      },
      "source": [
        "As you can see, the text needs to be cleaned up a bit. Below we use the `nltk` toolkit to remove spaces, html tags, stopwords, symbols etc. We define a function to remove stop words, replace / \\ and other symbols."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bydjpKCBQInv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c657280-e1ae-43a2-c7d1-86a373e70285"
      },
      "source": [
        "# note: slower students may wish to skip this step to finish the lab in class\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# load a list of stop words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        text: a string \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = BeautifulSoup(text, 'html.parser').text # HTML decoding\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        "    return text"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiiOIj3L_NDH"
      },
      "source": [
        "Now we can apply the newly defined function on the column of `df 'post'`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A8nlP0hQInx"
      },
      "source": [
        "df['post'] = df['post'].apply(clean_text)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkBB_8za_SE5"
      },
      "source": [
        "Let's check the results: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlL2uGKsQIn0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b202c2d-8634-4822-cc3e-3cf6302b811a"
      },
      "source": [
        "print(df['post'].values[10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "need interface c# possible duplicate would want use interfaces need interface want know use example interface idemo function prototype public void show first class using interface class myclass1 idemo public void show function body comes responsewrite myclass second class using interface class myclass2 idemo public void show function body comes responsewrite myclass2 responsewrite two classes function name different body even achieved without interface need interface use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTkqUfwzQIn8"
      },
      "source": [
        "This looks a lot better!\n",
        "\n",
        "Now how many unique words do we have in this cleaned up dataset? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oV5baXxQIn8",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838be002-4fbc-4696-9c06-174f8d4599c1"
      },
      "source": [
        "df['post'].apply(lambda x: len(x.split(' '))).sum()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3424194"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpllc3QyQIoA"
      },
      "source": [
        "Now we have over 3 million words to work with, that's 7 million removed tags.\n",
        "\n",
        "Before we start creating classifiers, let's split our dataset 70-30 in a test set (for evaluation) and training set: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylA7e4H_QIoB"
      },
      "source": [
        "X = df.post\n",
        "y = df.tags\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDHptCZqQIoU"
      },
      "source": [
        "#### Logistic regression\n",
        "\n",
        "Now that we have our features, we can train a classifier to try to predict the tag of a post. We will start with logistic regression and TFIDF representation which provides a nice baseline for this task. \n",
        "\n",
        "To make the vectorizer => transformer => classifier easier to work with, we will use the `Pipeline` class in Scikit-Learn that behaves like a compound classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8IMmMZWQIoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c7e04e-3b52-4fd6-c86e-8a1dc0ad3c50"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# we define a Pipeline, which first represents our features as TFID\n",
        "# Then performs logistic regression\n",
        "logreg = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
        "               ])\n",
        "logreg.fit(X_train, y_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('clf',\n",
              "                 LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=1, penalty='l2',\n",
              "                                    random_state=None, solver='lbfgs',\n",
              "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plgMq5lA27-_"
      },
      "source": [
        "How well does it work? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGSVTzWRQIoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "589748bd-41fd-4d40-ba3c-c6bc7eac5284"
      },
      "source": [
        "# to show the computation time: \n",
        "%%time\n",
        "\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.7861666666666667\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         .net       0.67      0.64      0.65       613\n",
            "      android       0.91      0.90      0.91       620\n",
            "    angularjs       0.97      0.93      0.95       587\n",
            "      asp.net       0.76      0.75      0.75       586\n",
            "            c       0.78      0.83      0.81       599\n",
            "           c#       0.61      0.59      0.60       589\n",
            "          c++       0.79      0.76      0.77       594\n",
            "          css       0.84      0.87      0.85       610\n",
            "         html       0.70      0.73      0.71       617\n",
            "          ios       0.61      0.59      0.60       587\n",
            "       iphone       0.65      0.63      0.64       611\n",
            "         java       0.83      0.82      0.83       594\n",
            "   javascript       0.77      0.79      0.78       619\n",
            "       jquery       0.85      0.85      0.85       574\n",
            "        mysql       0.82      0.85      0.83       584\n",
            "  objective-c       0.66      0.66      0.66       578\n",
            "          php       0.82      0.83      0.83       591\n",
            "       python       0.93      0.90      0.91       608\n",
            "ruby-on-rails       0.96      0.94      0.95       638\n",
            "          sql       0.79      0.85      0.82       601\n",
            "\n",
            "     accuracy                           0.79     12000\n",
            "    macro avg       0.79      0.79      0.79     12000\n",
            " weighted avg       0.79      0.79      0.79     12000\n",
            "\n",
            "CPU times: user 1.26 s, sys: 9.52 ms, total: 1.27 s\n",
            "Wall time: 1.26 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAAVJw44xn_c"
      },
      "source": [
        "That's quite a good accuracy. Now let's see if we can combine **word2vec** with logistic regression by feeding the new embedded representation to our logistic regression instead of the bag of words representation of TFIDF. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gSX1ysMQIoc"
      },
      "source": [
        "### Word2vec embedding and Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubl-sOB8W2f1"
      },
      "source": [
        "Let's load a pretrained word2vec model, and use the embedding representation as input to a simple classifier (i.e. logistic regression). \n",
        "\n",
        "You can use the word2vec model you trained in the first part of the lab (on the Shakespeare text), or load this (quite big, 1.5GB) pretrained word2vec model from Google trained on Google News data. \n",
        "\n",
        "If you load an model you trained yourself, use#\n",
        "`wv = gensim.models.KeyedVectors.load_word2vec_format(\"yourweights.bin.gz\", binary=True)`. We will be loading pretrained weights available in gensim itself:\n",
        "\n",
        "(This may take a while!)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjipngb9QIod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7de11d0-af84-43c9-f0cc-f652c14e2ae1"
      },
      "source": [
        "%%time\n",
        "import gensim.downloader\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", \n",
        "                                                    #  binary=True)\n",
        "\n",
        "wv = gensim.downloader.load('word2vec-google-news-300')\n",
        "wv.init_sims(replace=True)\n",
        "print('Model loaded')\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Model loaded\n",
            "CPU times: user 12min 34s, sys: 1min 43s, total: 14min 18s\n",
            "Wall time: 20min 35s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-W7lH8wxn_2"
      },
      "source": [
        "If you are interested how good these pretrained embeddings are, you could try some of the similarity tests we did in part 1 of the lab on the Shakespeare text. Only now we have a larger vocabulary, e.g.:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hyy5c5yC6XTx",
        "outputId": "cfbb28b7-1e1c-4cf5-dbcf-887893c0a5e6"
      },
      "source": [
        "wv.most_similar('twitter')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Twitter', 0.89089035987854),\n",
              " ('Twitter.com', 0.7536780834197998),\n",
              " ('tweet', 0.7431625723838806),\n",
              " ('tweeting', 0.7161933183670044),\n",
              " ('tweeted', 0.7137226462364197),\n",
              " ('facebook', 0.6988551616668701),\n",
              " ('tweets', 0.6974530816078186),\n",
              " ('Tweeted', 0.6950210928916931),\n",
              " ('Tweet', 0.6875007152557373),\n",
              " ('Tweeting', 0.6845167279243469)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3UmhwMo6f4g"
      },
      "source": [
        "Gensim offers a number of pretrained models for you to choose from (convenient right!). You can check a list of available model like this: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VIeyk0s6gCT",
        "outputId": "21de27d6-e96d-47bf-8d99-1330e4ef151e"
      },
      "source": [
        "# Show all available models in gensim-data\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC4XwSuzQIoo"
      },
      "source": [
        "As we have multiple words for each post, we will need to somehow combine them. A common way to achieve this is by averaging the\n",
        "word vectors per document. In later classes you can feed the individual words to memory models like LSTM. For a quick solution here, we can use a summation or weighted addition. The function below takes as input a list of words and the word2vec model `wv`. Then it retrieves the vector embeddings for each of the words and averages them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbSLtiwyQIoo"
      },
      "source": [
        "def word_averaging(wv, words):\n",
        "    # averages a set of words 'words' given their wordvectors 'wv'\n",
        "    \n",
        "    all_words, mean = set(), []\n",
        "    \n",
        "    # for each word in the list of words\n",
        "    for word in words:\n",
        "        # if the words are alread vectors, then just append them\n",
        "        if isinstance(word, np.ndarray):\n",
        "            mean.append(word)\n",
        "        # if not: first get the vector embedding for the words\n",
        "        elif word in wv.vocab:\n",
        "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
        "            all_words.add(wv.vocab[word].index)\n",
        "\n",
        "    \n",
        "    if not mean:\n",
        "        # error handling in case mean cannot be calculated\n",
        "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
        "        return np.zeros(wv.vector_size,)\n",
        "\n",
        "    # use gensim's method to calculate the mean of all the words appended to mean list\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def  word_averaging_list(wv, text_list):\n",
        "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm_febK-xoAC"
      },
      "source": [
        "Below, we explore a way (slightly different from the method used in part 1 of the lab) to create tokens out of sentences, by using the `nltk` toolkit. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlyXtYm1QIos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3599ef89-41e7-4fd1-8bec-e4e1a74ac862"
      },
      "source": [
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "\n",
        "def w2v_tokenize_text(text):\n",
        "    # create tokens, a list of words, for each post. This function will do some cleaning based on English language\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text, language='english'):\n",
        "        for word in nltk.word_tokenize(sent, language='english'):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word)\n",
        "    return tokens"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeuQn-3GxoAQ"
      },
      "source": [
        "Let's also split the dataset in training and test set like before, and tokenize each of these datasets using the method defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA1Wrn9-QIot"
      },
      "source": [
        "train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
        "\n",
        "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values\n",
        "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh11CM3ZxoAa"
      },
      "source": [
        "Since we have multiple word vectors per article, we can take multiple approaches (a powerful LSTM approach as we'll see later, or doc2vec as per below, but first we try a naive approach of averaging). We can average the word positions for each post in this new dataset using the functions we defined above and based on our word2vec model `wv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqG34rU6QIoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4f2f9c2-4aef-499e-91d7-ba3fd3153a4f"
      },
      "source": [
        "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
        "X_test_word_average = word_averaging_list(wv,test_tokenized)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
            "  del sys.path[0]\n",
            "WARNING:root:cannot compute similarity with no input []\n",
            "WARNING:root:cannot compute similarity with no input ['ngrepeat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zQa7btkxoAj"
      },
      "source": [
        "Now we have a way to represent our input! This can then be fed to any classifier, like logistic regression: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSSSuFQYQIo5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260b1bd4-7be8-42c0-a9c1-12dd2ef64414"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg = logreg.fit(X_train_word_average, train['tags'])\n",
        "y_pred = logreg.predict(X_test_word_average)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fy9vOz44h9p"
      },
      "source": [
        "Let's evaluate how accurate this averaged word2vec representation with logistic regression is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_WzSihIQIo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d2fb105-a2b4-4024-8b3f-49e73c63d369"
      },
      "source": [
        "print('accuracy %s' % accuracy_score(y_pred, test.tags))\n",
        "print(classification_report(test.tags, y_pred))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.6323333333333333\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         .net       0.63      0.56      0.59       613\n",
            "      android       0.76      0.76      0.76       620\n",
            "    angularjs       0.64      0.66      0.65       587\n",
            "      asp.net       0.54      0.50      0.52       586\n",
            "            c       0.70      0.73      0.71       599\n",
            "           c#       0.39      0.43      0.41       589\n",
            "          c++       0.63      0.63      0.63       594\n",
            "          css       0.73      0.78      0.75       610\n",
            "         html       0.57      0.62      0.60       617\n",
            "          ios       0.55      0.53      0.54       587\n",
            "       iphone       0.57      0.52      0.54       611\n",
            "         java       0.63      0.59      0.61       594\n",
            "   javascript       0.64      0.61      0.62       619\n",
            "       jquery       0.59      0.57      0.58       574\n",
            "        mysql       0.68      0.73      0.71       584\n",
            "  objective-c       0.41      0.40      0.41       578\n",
            "          php       0.67      0.70      0.68       591\n",
            "       python       0.80      0.76      0.78       608\n",
            "ruby-on-rails       0.83      0.81      0.82       638\n",
            "          sql       0.67      0.71      0.69       601\n",
            "\n",
            "     accuracy                           0.63     12000\n",
            "    macro avg       0.63      0.63      0.63     12000\n",
            " weighted avg       0.63      0.63      0.63     12000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNXuL03bxoAy"
      },
      "source": [
        "Now you can see that the accuracy went down! Oh no! Why is that? Because we used a very naive approach: averaging our vectors. A better way to approach this would be doc2vec, which learns relationships between documents (posts in this case), instead of words. The accuracy could also improve by using a different classifier instead of logistic regression, or by changing the aggregation strategy and feed it to an LSTM/RNN model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtujiFgzQIpA"
      },
      "source": [
        "## Doc2vec and Logistic Regression (advanced)\n",
        "\n",
        "The idea of word2vec can be extended to documents whereby instead of learning feature representations for words, we learn it for sentences or documents. Doc2Vec extends the idea of word2vec, however words can only capture so much, there are times when we need relationships between documents and not just words.\n",
        "\n",
        "The way to train doc2vec model for our Stack Overflow questions and tags data is very similar to when we trained multi-class text classification with word2vec and logistic regression above.\n",
        "\n",
        "First, we label the sentences. Gensim’s Doc2Vec implementation requires each document/paragraph to have a label associated with it that indicates if it's part of the test or training set. We do this by using the TaggedDocument method. The format will be `TRAIN_i` or `TEST_i` where `i` is a dummy index of the post.\n",
        "\n",
        "First let's import the necessary libraries. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkXdv0A6QIpB"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from gensim.models import doc2vec\n",
        "from sklearn import utils\n",
        "import gensim\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import re"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBov76MXxoA8"
      },
      "source": [
        "Let's start by defining a function that labels our documents in the corpus. We just give them dummy labels TRAIN_i or TEST_i for post i. Given a corpus and labels, we return a variable that includes a label indicating if it's test or training data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtVzwM8RQIpD"
      },
      "source": [
        "def label_sentences(corpus, label_type):\n",
        "    \"\"\"\n",
        "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
        "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
        "    a dummy index of the post.\n",
        "    \"\"\"\n",
        "    labeled = []\n",
        "    for i, v in enumerate(corpus):\n",
        "        label = label_type + '_' + str(i)\n",
        "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
        "    return labeled"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lsDfgBnxoBC"
      },
      "source": [
        "Just like above we split our dataset up in test and training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfXghWoJQIpF"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.post, df.tags, random_state=0, \n",
        "                                                    test_size=0.3)\n",
        "X_train = label_sentences(X_train, 'Train')\n",
        "X_test = label_sentences(X_test, 'Test')\n",
        "all_data = X_train + X_test"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_KYhbHmxoBI"
      },
      "source": [
        "Let's have a look how our data looks at this moment: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "321apZFWQIpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "798d0834-9ff8-4deb-adb9-1c11834565d5"
      },
      "source": [
        "all_data[:10]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TaggedDocument(words=['fulltext', 'search', 'php', 'pdo', 'returning', 'result', 'searched', 'lot', 'matter', 'find', 'wrong', 'setup', 'trying', 'fulltext', 'search', 'using', 'pdo', 'php', 'get', 'results', 'error', 'messages', 'table', 'contains', 'customer', 'details', 'id', 'int', '11', 'auto_increment', 'name', 'varchar', '150', 'lastname', 'varchar', '150', 'company', 'varchar', '250', 'adress', 'varchar', '150', 'postcode', 'int', '5', 'city', 'varchar', '150', 'email', 'varchar', '250', 'phone', 'varchar', '20', 'orgnr', 'varchar', '15', 'timestamp', 'timestamp', 'current_timestamp', 'run', 'sqlquery', 'alter', 'table', 'system_customer', 'add', 'fulltext', 'name', 'lastname', 'except', 'columns', 'id', 'postcode', 'timestamp', 'signs', 'trouble', 'far', 'idea', 'problem', 'lies', 'db', 'configuration', 'php', 'code', 'goes', 'php', 'sth', 'dbhprepare', 'select', 'name', 'lastname', 'company', 'adress', 'city', 'phone', 'email', 'orgnr', 'db_pre', 'customer', 'match', 'name', 'lastname', 'company', 'adress', 'city', 'phone', 'email', 'orgnr', 'search', 'boolean', 'mode', 'bind', 'placeholders', 'sthbindparam', 'search', 'data', 'sthexecute', 'rows', 'sthfetchall', 'testing', 'print_r', 'dbherrorinfo', 'empty', 'rows', 'echo', 'else', 'echo', 'foreach', 'rows', 'row', 'echo', 'tr', 'datahref', 'new_orderphp', 'cid', 'row', 'id', 'echo', 'td', 'row', 'name', 'td', 'echo', 'td', 'row', 'lastname', 'td', 'echo', 'td', 'row', 'company', 'td', 'echo', 'td', 'row', 'phone', 'td', 'echo', 'td', 'row', 'email', 'td', 'echo', 'td', 'date', 'ymd', 'strtotime', 'row', 'timestamp', 'td', 'echo', 'tr', 'echo', 'tried', 'change', 'parameter', 'searchquery', 'string', 'like', 'testcompany', 'somename', 'boolean', 'mode', 'also', 'read', 'word', 'found', '50', 'rows', 'counts', 'common', 'word', 'pretty', 'sure', 'case', 'uses', 'specific', 'words', 'table', 'uses', 'myisam', 'engine', 'get', 'results', 'error', 'messages', 'please', 'help', 'point', 'wrong', 'thank'], tags=['Train_0']),\n",
              " TaggedDocument(words=['select', 'everything', '1', 'table', 'x', 'rows', 'another', 'im', 'making', 'join', 'query', 'like', 'select', 'clothes', 'c', 'join', 'style', 'cstyleid', 'ssylelid', 'clothesid', '19', 'dont', 'want', 'select', 'everything', 'style', 'want', 'select', 'everything', 'clothes', '20', 'rows', 'select', '1', 'row', '10', 'style', 'easyest', 'way', 'without', 'select', 'every', 'row', 'clothes', '20', 'things', 'select', 'like', 'select', 'cid', 'cdescription', 'cname', 'csize', 'cbrand', 'sname', 'clothes', 'c', 'join', 'style', 'cstyleid', 'stsylelid', 'clothesid', '19', 'would', 'fastest', 'way', 'possibillity'], tags=['Train_1']),\n",
              " TaggedDocument(words=['r', 'cannot', 'resolved', 'variable', 'importing', 'project', 'pasting', 'problems', 'details', 'r', 'cannot', 'resolved', 'variable', 'common', 'problem', 'checked', 'res', 'folder', 'done', 'refreshing', 'project', 'cleaning', 'project', 'validate', 'still', 'error', 'resolved', 'help', 'guys', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'resdrawableic_launcher_wallpaperpng0', 'error', 'resource', 'entry', 'ic_launcher_wallpaper', 'already', 'defined', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'resdrawableic_launcher_wallpaperhtml0', 'originally', 'defined', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'resxmlcube1xml0', 'error', 'resource', 'entry', 'cube1', 'already', 'defined', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'resxmlcube1html0', 'originally', 'defined', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'resxmlcube2xml0', 'error', 'resource', 'entry', 'cube2', 'already', 'defined', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'resxmlcube2html0', 'originally', 'defined', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'resxmlcube2_settingsxml0', 'error', 'resource', 'entry', 'cube2_settings', 'already', 'defined', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'resxmlcube2_settingshtml0', 'originally', 'defined', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'fsample', 'projectscuberesvaluesindexhtml112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'fsample', 'projectscuberesvaluesshapeshtml112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'fsample', 'projectscuberesvaluesstringshtml112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'fsample', 'projectscuberesxmlcube1html112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'fsample', 'projectscuberesxmlcube2html112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'fsample', 'projectscuberesxmlcube2_settingshtml112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '20121214', '021238', 'comexampleandroidlivecubescube2cubewallpaper2settings', 'fsample', 'projectscuberesxmlindexhtml112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', 'copyright', 'c', '2009', 'google', 'inc', 'licensed', 'apache', 'license', 'version', '20', 'license', 'may', 'use', 'file', 'except', 'compliance', 'license', 'may', 'obtain', 'copy', 'license', 'http', 'wwwapacheorg', 'licenses', 'license20', 'unless', 'required', 'applicable', 'law', 'agreed', 'writing', 'software', 'distributed', 'license', 'distributed', 'basis', 'without', 'warranties', 'conditions', 'kind', 'either', 'express', 'implied', 'see', 'license', 'specific', 'language', 'governing', 'permissions', 'limitations', 'license', 'package', 'comexampleandroidlivecubescube2', 'import', 'comexampleandroidlivecubescube2', 'import', 'androidcontentsharedpreferences', 'import', 'androidosbundle', 'import', 'androidpreferencepreferenceactivity', 'public', 'class', 'cubewallpaper2settings', 'extends', 'preferenceactivity', 'implements', 'sharedpreferencesonsharedpreferencechangelistener', 'override', 'protected', 'void', 'oncreate', 'bundle', 'icicle', 'superoncreate', 'icicle', 'getpreferencemanager', 'setsharedpreferencesname', 'cubewallpaper2shared_prefs_name', 'addpreferencesfromresource', 'rxmlcube2_settings', 'getpreferencemanager', 'getsharedpreferences', 'registeronsharedpreferencechangelistener', 'override', 'protected', 'void', 'onresume', 'superonresume', 'override', 'protected', 'void', 'ondestroy', 'getpreferencemanager', 'getsharedpreferences', 'unregisteronsharedpreferencechangelistener', 'superondestroy', 'public', 'void', 'onsharedpreferencechanged', 'sharedpreferences', 'sharedpreferences', 'string', 'key'], tags=['Train_2']),\n",
              " TaggedDocument(words=['efficient', 'way', 'get', 'values', 'object', 'based', 'id', 'list', 'list', 'users', 'users', 'arrayany', 'id1', 'name', 'id2', 'name', 'b', 'id3', 'name', 'c', 'thisselectedusers', '1', '2', 'get', 'objects', 'users', 'array', 'id', 'found', 'selectusers', 'array'], tags=['Train_3']),\n",
              " TaggedDocument(words=['aspnet', 'limit', 'parameter', 'length', 'querystring', 'problem', 'passing', 'parameters', 'querystring', 'found', 'values', 'null', 'code', 'snippet', 'page1', 'passing', 'parameters', 'responseredirect', 'stringformat', 'requestreservationpageaspx', 'plcname', '0', 'plcindex', '1', 'email', '2', 'form', '3', '4', 'sr', '5', 'comment', '6', 'lblplcnamevaltext', 'index', 'lblemailvaltext', 'datetimeparse', 'lblreqfromvaltext', 'toshortdatestring', 'datetimeparse', 'lblreqtovaltext', 'toshortdatestring', 'lblservreqnumtext', 'lblyourcommentvaltext', 'page2', 'requesting', 'values', 'cmbplcrequestselectedindex', 'converttoint32', 'requestquerystring', 'plcindex', 'txtemailtext', 'converttostring', 'requestquerystring', 'email', 'txtsrtext', 'converttostring', 'requestquerystring', 'sr', 'txtcommenttext', 'converttostring', 'requestquerystring', 'comment', 'txtreqfromdatetext', 'requestquerystring', 'txtreqtodatetext', 'requestquerystring', 'found', 'requestquerystring', 'requestquerystring', 'return', 'null', 'idea'], tags=['Train_4']),\n",
              " TaggedDocument(words=['ruby', 'rails', 'fetch', 'display', 'descendent', 'records', 'parent', 'model', 'many', 'children', 'class', 'band', 'activerecordbase', 'has_many', 'concerts', 'end', 'class', 'concerts', 'activerecordbase', 'belongs_to', 'band', 'end', 'would', 'like', 'display', 'index', 'view', 'figure', 'syntax', 'displaying', 'children', 'records', 'bandseach', 'band', 'band', 'name', 'bandname', 'concerts', 'ul', 'bandsconcertseach', 'concert', 'concertlocation', 'end', 'ul', 'end', 'getting', 'error', 'like', 'undefined', 'method', 'concerts', '#array0x00000102c537f0', 'proper', 'way', 'fetching', 'displaying', 'descendent', 'models'], tags=['Train_5']),\n",
              " TaggedDocument(words=['canceling', 'fade', 'effect', 'tooltip', 'hover', 'need', 'tooltip', 'hyperlinks', 'inside', 'web', 'site', 'wrote', 'code', 'seems', 'working', 'fine', 'expect', 'one', 'problem', 'hover', 'tooltip', 'block', 'fades', 'fades', 'need', 'prevent', 'fadeout', 'hovet', 'somehow', 'tried', 'use', 'stop', 'method', 'work', 'probably', 'something', 'wrong', 'could', 'please', 'help', 'thanks', 'html', 'div', 'id', 'hover', 'div', 'class', 'tooltip', 'href', '#', 'href', '#', 'href', '#', 'div', 'div', 'css', 'body', 'margin', '0', 'padding', '0', 'width', '100', 'height', '100', '#hover', 'position', 'relative', 'width50px', 'height50px', 'background', 'green', 'tooltip', 'position', 'relative', 'width', '45px', 'top', '80px', 'height', '20px', 'border', '1px', 'solid', 'black', 'padding', '5px', 'display', 'inlineblock', 'margintop', '5px', 'height', '10px', 'width', '10px', 'border', '1px', 'solid', 'black', 'background', 'red', 'jquery', 'document', 'ready', 'function', 'tooltip', 'hide', '#hover', 'hover', 'function', 'find', 'tooltip', 'fadein', 'function', 'find', 'tooltip', 'delay', '1000', 'fadeout', 'demo', 'http', 'jsfiddlenet', '8gc3d', '2904'], tags=['Train_6']),\n",
              " TaggedDocument(words=['ajax', 'calender', 'working', 'ie', 'using', 'ajax', 'calender', 'readonly', 'textbox', 'control', 'select', 'date', 'click', 'date', 'calender', 'picks', 'date', 'attach', 'txtfromdate', 'working', 'correctly', 'ff', 'chrome', 'ie', 'code', 'asptextbox', 'id', 'txtfromdate', 'text', 'date', 'runat', 'server', 'onfocus', 'javascriptthisvalue', 'onblur', 'javascript', 'thisvalue', 'thisvalue', 'date', 'asptextbox', 'ajaxcalendarextender', 'id', 'txtcalendecontrolextenderfromdate', 'runat', 'server', 'format', 'ddmmmyyyy', 'targetcontrolid', 'txtfromdate', 'ajaxcalendarextender'], tags=['Train_7']),\n",
              " TaggedDocument(words=['c++', 'random', 'number', 'generator', 'hung', 'whenever', 'attempt', 'run', 'code', 'program', 'gets', 'hung', 'print', 'value', 'r2eff', 'sure', 'help', 'would', 'appreciated', 'sample', 'value', 'r2efftemp', 'would', '10', 'stddev', 'would', '05', '5', 'unsigned', 'seed', 'stdchronosystem_clocknow', 'time_since_epoch', 'count', 'stdmt19937', 'generator', 'seed', 'normal_distributiondouble', 'rand', 'r2efftemp', 'r2efftempstddev', 'r2efftemp', 'rand', 'generator', 'coutr2efftempendl', 'thank'], tags=['Train_8']),\n",
              " TaggedDocument(words=['bit', 'vector', 'looked', 'online', 'good', 'seem', 'find', 'good', 'example', 'bit', 'vector', 'actually', 'assignment', 'college', 'add', 'remove', 'union', '2', 'vectors', 'intersection', 'struggling', 'comprehend', 'actual', 'bit', 'vector', 'using', 'c', 'write', 'could', 'someone', 'please', 'help', 'would', 'massive', 'help'], tags=['Train_9'])]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvY-IsHNxoBQ"
      },
      "source": [
        "Gensim allows us to build a model very easily. We can vary the parameters to fit your data: \n",
        "\n",
        "*    `dm=0` , distributed bag of words (DBOW) is used.\n",
        "*    `vector_size=300` , 300 vector dimensional feature vectors.\n",
        "*    `negative=5` , specifies how many “noise words” should be drawn.\n",
        "*    `min_count=1`, ignores all words with total frequency lower than this.\n",
        "*    `alpha=0.065` , the initial learning rate.\n",
        "\n",
        "We initialize the model and train for 30 epochs. (Those of you on slower computers may want to train for less epochs). Be sure to set your runtime to GPU hardware acceleration! Maybe test with a lower amount of epochs first to see how high you can go during class time!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UoqpKnQIpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a3760b3-bdfa-47bd-ebba-9c7c0150642f"
      },
      "source": [
        "model_dbow = doc2vec.Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, \n",
        "                     min_alpha=0.065)\n",
        "model_dbow.build_vocab([x for x in tqdm(all_data)])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40000/40000 [00:00<00:00, 2112919.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSDy4huyQIpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "991f259a-ef6e-4684-f4ac-04a07704c213"
      },
      "source": [
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), \n",
        "                     total_examples=len(all_data), \n",
        "                     epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40000/40000 [00:00<00:00, 2085291.90it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 1975696.08it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2263948.40it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2817995.16it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2209388.96it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2827493.60it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2101144.17it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2163629.52it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2143423.15it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 1837250.02it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 3001935.30it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2168215.25it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2239560.02it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 1981763.80it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2860419.07it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2415934.57it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2912357.18it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2278986.65it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2224534.40it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2354531.75it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2097492.84it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2226039.70it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 1342166.54it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2173636.85it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2273027.50it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 1484893.35it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2250132.91it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2032887.35it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2135538.30it/s]\n",
            "100%|██████████| 40000/40000 [00:00<00:00, 2689389.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNp4a3ouxoB5"
      },
      "source": [
        "Now let's define a function to get the vector of a particular word from this trained  model, so that we can feed them into the logistic regression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6cGBFuRQIpT"
      },
      "source": [
        "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
        "    \"\"\"\n",
        "    Get vectors from trained doc2vec model\n",
        "    :param doc2vec_model: Trained Doc2Vec model\n",
        "    :param corpus_size: Size of the data\n",
        "    :param vectors_size: Size of the embedding vectors\n",
        "    :param vectors_type: Training or Testing vectors\n",
        "    :return: list of vectors\n",
        "    \"\"\"\n",
        "    vectors = np.zeros((corpus_size, vectors_size))\n",
        "    for i in range(0, corpus_size):\n",
        "        prefix = vectors_type + '_' + str(i)\n",
        "        vectors[i] = model.docvecs[prefix]\n",
        "    return vectors"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgOEdSpS7nTW"
      },
      "source": [
        "We can use this function to create a vectorised training and test set with 1 entry per document for the input in classification models such as logistic regression. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57wLrRE3QIpX"
      },
      "source": [
        "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
        "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJE2ZeuOxoCD"
      },
      "source": [
        "We can now feed these vectors to the classifier again: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoLo0XkeQIpa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "664b7326-ac47-49ee-9429-218f6d385f3c"
      },
      "source": [
        "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg.fit(train_vectors_dbow, y_train)\n",
        "\n",
        "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
        "y_pred = logreg.predict(test_vectors_dbow)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.7975\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         .net       0.67      0.66      0.67       589\n",
            "      android       0.89      0.91      0.90       661\n",
            "    angularjs       0.94      0.96      0.95       606\n",
            "      asp.net       0.77      0.78      0.78       613\n",
            "            c       0.84      0.88      0.86       601\n",
            "           c#       0.72      0.70      0.71       585\n",
            "          c++       0.86      0.80      0.83       621\n",
            "          css       0.82      0.83      0.83       587\n",
            "         html       0.68      0.66      0.67       560\n",
            "          ios       0.68      0.69      0.68       611\n",
            "       iphone       0.66      0.63      0.65       593\n",
            "         java       0.81      0.84      0.82       581\n",
            "   javascript       0.77      0.78      0.78       608\n",
            "       jquery       0.85      0.82      0.84       593\n",
            "        mysql       0.82      0.80      0.81       592\n",
            "  objective-c       0.68      0.66      0.67       597\n",
            "          php       0.83      0.85      0.84       604\n",
            "       python       0.90      0.92      0.91       610\n",
            "ruby-on-rails       0.93      0.94      0.94       595\n",
            "          sql       0.78      0.80      0.79       593\n",
            "\n",
            "     accuracy                           0.80     12000\n",
            "    macro avg       0.80      0.80      0.80     12000\n",
            " weighted avg       0.80      0.80      0.80     12000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqmumA0-xoCV"
      },
      "source": [
        "We get 80%, that is the best result so far! Remember, we can actually use any classifier with this method! So up to you to make your project as efficient as possible :)\n",
        "    \n",
        "Try using a different classifiers, e.g. Decision tree or SVM. Does that influence the results? \n",
        "\n",
        "New methods are coming out every day in the field of data science. Just at the end of August 2019, the first implementation of BERT for document classfication was published: DocBERT: https://arxiv.org/abs/1904.08398\n",
        "\n",
        "These embeddings can similarly be loaded. There are also specialised pretrainend embeddings for say, financial data, e.g. FinBERT. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvthUI_yxoCg"
      },
      "source": [
        "## References\n",
        "\n",
        "* https://radimrehurek.com/gensim/models/word2vec.html\n",
        "* https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
        "* https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/word2vec\n",
        "* https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDmjCi1QHIkV"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Now over to you! \n",
        "\n",
        "Can you develop a doc2vec with SVM classifier for the following dataset? \n",
        "\n",
        "https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset?select=Fake.csv\n",
        "\n",
        "The task is to predict if news is fake or real. \n",
        "\n",
        "As input, use only the text for simplicity (possibly concatenated with title, but not necessary). \n",
        "\n",
        "Good luck! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QyRLoutHY3g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "cb6e76f7-7139-4e95-e2dc-2283c8430192"
      },
      "source": [
        "## solution\n",
        "!pip install wget\n",
        "import wget\n",
        "wget.download(\"https://dorienherremans.com/drop/CDS/word2vec/fake.zip\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=fb1eb24ac87616fb10a3817936b254a5cda5c5fccd0ab0a692b44dc51032cdf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'fake.zip'"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKp0Wex8O9rM",
        "outputId": "2d7c857b-af5f-4d4f-f5ec-60353954f2ff"
      },
      "source": [
        "!unzip fake.zip"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  fake.zip\n",
            "  inflating: Fake.csv                \n",
            "  inflating: True.csv                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KySVEO0XPM2A"
      },
      "source": [
        "fake_df = pd.read_csv(\"Fake.csv\")\n",
        "true_df = pd.read_csv(\"True.csv\")\n",
        "\n",
        "fake_df[\"true\"] = 0\n",
        "true_df[\"true\"] = 1\n",
        "\n",
        "df = pd.concat([fake_df, true_df])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "41_hb3jwPZmd",
        "outputId": "935b81ef-55c0-43b1-d999-75edc76f2610"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>true</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
              "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 30, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
              "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 25, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... true\n",
              "0   Donald Trump Sends Out Embarrassing New Year’...  ...    0\n",
              "1   Drunk Bragging Trump Staffer Started Russian ...  ...    0\n",
              "2   Sheriff David Clarke Becomes An Internet Joke...  ...    0\n",
              "3   Trump Is So Obsessed He Even Has Obama’s Name...  ...    0\n",
              "4   Pope Francis Just Called Out Donald Trump Dur...  ...    0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8qYBL6fPcII",
        "outputId": "30e343d5-c3a0-4e49-bf34-6788918af1a1"
      },
      "source": [
        "df['text'] = df['text'].apply(clean_text)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://100percentfedup.com/served-roy-moore-vietnamletter-veteran-sets-record-straight-honorable-decent-respectable-patriotic-commander-soldier/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=cJZFepSvxzM\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=-7Tn4gi_Os8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?time_continue=2&v=IjWClQcKhD8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=SH0pRtK9sAE\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=DRLVvYzG46w\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=Ws5ojb0PCCo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=P-TBfkqk7gU\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=n9tfNMQpYWU\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=PjeOoJyPNCk\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=hNPX8ZCIfc0&t=26s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=J4LjxrOfEF8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=uCS4RB9G13M\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?time_continue=1&v=NeqMSI6OR5Q\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=ISm-p8e-D7I\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=VkRCtn0nEvU\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=8Mehk5eWcZA\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=rUr8pYr5AXs\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=zZ7GrEItGoo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?time_continue=139&v=Iil1Z8-A1GA\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=gqxwF-TeYas\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=aHkNzBRqPCE\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=EOr9fwoc_mo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=9LNyx_DWzzA\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=RRPSCqkAJgk\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=tY0ApLE6dns\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=sMHGkzrwzKg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=6VN1maBEKIk\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=8dsDdBqF828\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=IioEIUmawRo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=31MRqr9ydUU\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=wYdX071Nlow\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=YeDU6dCR9tA\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=0cVugq2GbBk\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=CCr0qvehJIk\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=HXJZbPAf0sk\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?feature=player_embedded&v=JebHe3049aA\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=sWbYpIj7CQ8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://youtu.be/7oOhwHG2Gb4\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=_FNt3ns_EGA\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://youtu.be/RTuxvWjH3a4\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=uQbAww5wajA\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://youtu.be/Ai5ayloRa-0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://twitter.com/Rosie/status/800939338615824384\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://youtu.be/0J4xPRYbsLU\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://youtu.be/kKFQ5i9jXmA\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=1RVqTfIKGbU\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://100percentfedup.com/video-hillary-asked-about-trump-i-just-want-to-eat-some-pie/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://100percentfedup.com/12-yr-old-black-conservative-whose-video-to-obama-went-viral-do-you-really-love-america-receives-death-threats-from-left/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://fedup.wpengine.com/wp-content/uploads/2015/04/hillarystreetart.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://fedup.wpengine.com/wp-content/uploads/2015/04/entitled.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.youtube.com/watch?v=IPqrimR8GWw\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1btr0BV6R4Yv"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['true'], random_state=0,test_size=0.3)\n",
        "X_train = label_sentences(X_train, 'Train')\n",
        "X_test = label_sentences(X_test, 'Test')\n",
        "all_data = X_train + X_test"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBT0A5p2SPIL",
        "outputId": "738dd8de-b294-4a36-ec60-3379841ced16"
      },
      "source": [
        "model_dbow = doc2vec.Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, \n",
        "                     min_alpha=0.065)\n",
        "model_dbow.build_vocab([x for x in tqdm(all_data)])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44898/44898 [00:00<00:00, 2373380.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sertrZuoVf1F",
        "outputId": "1fc5e34f-da6d-40ba-fac1-2c5c6d47f196"
      },
      "source": [
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), \n",
        "                     total_examples=len(all_data), \n",
        "                     epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44898/44898 [00:00<00:00, 2351479.21it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2131789.18it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 1958318.89it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2235786.92it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2284444.05it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2353918.84it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2193264.24it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2215637.12it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2112348.41it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2865557.79it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2220365.52it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2422411.67it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2184056.01it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2168388.42it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2835996.82it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2291114.45it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2313206.91it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2804907.22it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2040789.17it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2197538.46it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2289832.94it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2159115.11it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2522549.14it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2334889.73it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2184866.88it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2275857.89it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2070656.56it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2286913.12it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2290278.52it/s]\n",
            "100%|██████████| 44898/44898 [00:00<00:00, 2197769.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6ARaIibSTs6"
      },
      "source": [
        "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
        "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JnCf3qnSVi1",
        "outputId": "afa55783-75b1-44c3-b873-2139698f1bd2"
      },
      "source": [
        "from sklearn.svm import SVC \n",
        "\n",
        "svc_model = SVC(C=1e5)\n",
        "svc_model.fit(train_vectors_dbow, y_train)\n",
        "y_pred = svc_model.predict(test_vectors_dbow) \n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.9692650334075724\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.97      7025\n",
            "           1       0.97      0.96      0.97      6445\n",
            "\n",
            "    accuracy                           0.97     13470\n",
            "   macro avg       0.97      0.97      0.97     13470\n",
            "weighted avg       0.97      0.97      0.97     13470\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ae9IHanSbW2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}